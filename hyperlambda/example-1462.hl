
// Write a Hyperlambda script for commonalities between different endpoints handling authentication, reCAPTCHA, and other common tasks.
slots.create:magic.ai.endpoint-common

   // Sanity checking invocation.
   validators.mandatory:x:@.arguments/*/type
   validators.mandatory:x:@.arguments/*/prompt
   validators.string:x:@.arguments/*/prompt
      min:1

   // Trimming prompt.
   set-value:x:@.arguments/*/prompt
      strings.trim:x:@.arguments/*/prompt

   // Buffer for model settings.
   .model

   // Connecting to database to retrieve model settings.
   data.connect:[generic|magic]

      // Reading settings for type.
      data.read
         table:ml_types
         columns
            model
            max_tokens
            max_context_tokens
            max_request_tokens
            temperature
            recaptcha
            auth
            supervised
            cached
            prefix
            use_embeddings
            threshold
            vector_model
            contact_us
            lead_email
            api_key
            webhook_incoming
            webhook_outgoing
            webhook_incoming_url
            webhook_outgoing_url
            system_message
            initial_questionnaire
            no_requests
            max_requests
            session_timeout
            search_postfix
            max_function_invocations
            max_session_items
            completion_slot
         where
            and
               type.eq:x:@.arguments/*/type

      // Verifying type exists.
      if
         not-exists:x:@data.read/*
         .lambda

            // Oops, no such type, trying to see if default type exists.
            data.read
               table:ml_types
               columns
                  model
                  max_tokens
                  max_context_tokens
                  max_request_tokens
                  temperature
                  recaptcha
                  auth
                  supervised
                  cached
                  prefix
                  use_embeddings
                  threshold
                  vector_model
                  contact_us
                  lead_email
                  api_key
                  webhook_incoming
                  webhook_outgoing
                  webhook_incoming_url
                  webhook_outgoing_url
                  system_message
                  initial_questionnaire
                  no_requests
                  max_requests
                  session_timeout
                  search_postfix
                  max_function_invocations
                  max_session_items
                  completion_slot
               where
                  and
                     type.eq:default
            if
               not-exists:x:@data.read/*
               .lambda

                  // Default type doesn't exist, nothing to do here.
                  throw:No such type, and no default type was found
                     status:int:400
                     public:bool:true

            // Resorting to default type.
            add:x:@.model
               get-nodes:x:@data.read/*/*
            set-value:x:@.arguments/*/type
               .:default
      else

         // Model exists, populating above buffer.
         add:x:@.model
            get-nodes:x:@data.read/*/*

   /*
    * Checking if model requires authentication and authorisation,
    * unless caller explicitly informs us he wants to skip authentication.
    */
   if
      or
         not-exists:x:@.arguments/*/skip-auth
         eq:x:@.arguments/*/skip-auth
            .:bool:false
      .lambda

         // Caller wants default authentication logic, first checking if model requires authentication.
         if
            and
               not-null:x:@.model/*/auth
               neq:x:@.model/*/auth
                  .:
            .lambda

               // Making sure user is authorised to using type.
               auth.ticket.verify:x:@.model/*/auth

         // Checking if model requires reCAPTCHA.
         if
            and
               eq
                  auth.ticket.get
                  .
               mt
                  convert:x:@.model/*/recaptcha
                     type:decimal
                  .:decimal:0
               not-null:x:@.model/*/recaptcha
            .lambda

               // Verifying reCAPTCHA was supplied.
               if
                  or
                     not-exists:x:@.arguments/*/recaptcha_response
                     null:x:@.arguments/*/recaptcha_response
                  .lambda

                     // Endpoint invoked without reCAPTCHA, making sure we abort invocation.
                     response.status.set:499
                     return
                        error:No reCAPTCHA supplied

               // Retrieving reCAPTCHA site key.
               .key
               set-value:x:@.key
                  config.get:"magic:auth:recaptcha:key"

               // Retrieving reCAPTCHA secret.
               .secret
               set-value:x:@.secret
                  config.get:"magic:auth:recaptcha:secret"

               // Validating reCAPTCHA invocation confirms request originated from a human.
               convert:x:@.model/*/recaptcha
                  type:decimal
               validators.recaptcha:x:@.arguments/*/recaptcha_response
                  min:x:@convert
                  site-key:x:@.key
                  secret:x:@.secret
         else-if
            and
               eq
                  auth.ticket.get
                  .
               not
                  lt
                     convert:x:@.model/*/recaptcha
                        type:decimal
                     .:decimal:0
            .lambda

               // Using built-in CAPTCHA.
               execute:magic.auth.captcha-verify
                  token:x:@.arguments/*/recaptcha_response

   // Checking if we've reached [max_requests] on model.
   if
      and
         neq:x:@.model/*/max_requests
            .:long:-1
         mte
            get-value:x:@.model/*/no_requests
            get-value:x:@.model/*/max_requests
      .lambda

         // Oops, we've reached maximum number of requests for model this month.
         throw:Maximum number of requests reached for your model this month
            public:bool:true
            status:int:500

   // Applying mixin on model's system message, if it contains mixin logic.
   if
      strings.contains:x:@.model/*/system_message
         .:{{
      .lambda

         // Applying mixin logic.
         set-value:x:@.model/*/system_message
            strings.mixin:x:@.model/*/system_message

   // Checking if we've configured integrations for incoming messages.
   .incoming
   set-value:x:@.incoming
      get-first-value
         get-value:x:@.model/*/webhook_incoming
         config.get:"magic:openai:integrations:incoming:slot"
   if
      and
         not-null:x:@.incoming
         neq:x:@.incoming
            .:
      .lambda

         // Invoking integration slot.
         .exe

            // Retrieving URL to invoke.
            .hook-url
            set-value:x:@.hook-url
               get-first-value
                  get-value:x:@.model/*/webhook_incoming_url
                  config.get:"magic:openai:integrations:incoming:url"

            // Invoking slot.
            unwrap:x:./*/signal/*/url
            signal:x:@.incoming
               url:x:@.hook-url

         // Parametrizing invocation to integration slot.
         if
            and
               exists:x:@.arguments/*/to
               exists:x:@.arguments/*/from
               not-null:x:@.arguments/*/to
               not-null:x:@.arguments/*/from
               strings.contains:x:@.arguments/*/to
                  .:":"
               strings.contains:x:@.arguments/*/from
                  .:":"
            .lambda

               // We have a channel to accommodate for.
               .channel
               .to
               .from
               strings.split:x:@.arguments/*/to
                  .:":"
               set-value:x:@.channel
                  get-value:x:@strings.split/0
               set-value:x:@.to
                  get-value:x:@strings.split/1
               strings.split:x:@.arguments/*/from
                  .:":"
               set-value:x:@.from
                  get-value:x:@strings.split/1
               add:x:@.exe/*/signal
                  get-nodes:x:@.arguments/*/prompt
                  get-nodes:x:@.arguments/*/session
               unwrap:x:+/*/*
               add:x:@.exe/*/signal
                  .
                     to:x:@.to
                     from:x:@.from
                     channel:x:@.channel
         else

            // No channel
            add:x:@.exe/*/signal
               get-nodes:x:@.arguments/*/to
               get-nodes:x:@.arguments/*/from
               get-nodes:x:@.arguments/*/prompt
               get-nodes:x:@.arguments/*/session

         // Invoking callback.
         eval:x:@.exe

   // Doing some common conversions.
   set-value:x:@.model/*/threshold
      convert:x:@.model/*/threshold
         type:double
   set-value:x:@.model/*/max_tokens
      convert:x:@.model/*/max_tokens
         type:int
   set-value:x:@.model/*/max_context_tokens
      convert:x:@.model/*/max_context_tokens
         type:int
   set-value:x:@.model/*/max_request_tokens
      convert:x:@.model/*/max_request_tokens
         type:int

   // Making sure se set max size of model
   switch:x:@.model/*/model
      case:gpt-4o
      case:gpt-4o-mini
      case:gpt-4o-2024-05-13
      case:gpt-4-turbo
      case:gpt-4-0125-preview
      case:gpt-4-1106-preview
      case:gpt-4-turbo-preview
      case:gpt-4-turbo-2024-04-09
      case:gpt-4-vision-preview
      case:gpt-4-1106-vision-preview
         add:x:@.model
            .
               model_size:int:128000
      case:gpt-4
      case:gpt-4-0613
         add:x:@.model
            .
               model_size:int:8192
      case:gpt-4-32k
      case:gpt-4-32k-0613
         add:x:@.model
            .
               model_size:int:32768
      case:gpt-3.5-turbo
      case:gpt-3.5-turbo-0125
      case:gpt-3.5-turbo-1106
      case:gpt-3.5-turbo-16k
      case:gpt-3.5-turbo-16k-0613
         add:x:@.model
            .
               model_size:int:16384
      case:gpt-3.5-turbo-0613
      case:gpt-3.5-turbo-instruct
         add:x:@.model
            .
               model_size:int:4096
      case:o3-mini
         add:x:@.model
            .
               model_size:int:200000
      case:gpt-4.5-preview
         add:x:@.model
            .
               model_size:int:128000

   // Adding defaults in case model has not been configured with max_request_tokens
   if
      or
         null:x:@.model/*/max_context_tokens
         null:x:@.model/*/max_request_tokens
      .lambda

         /*
          * Defaulting [max_request_tokens] and [max_context_tokens] according
          * to what model we're using.
          */
         switch:x:@.model/*/model
            case:text-davinci-003
            case:gpt-3.5-turbo
            case:gpt-3.5-turbo-0301
            case:text-davinci-002
               math.divide
                  math.subtract:int:4096
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:code-davinci-002
               math.divide
                  math.subtract:int:8000
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4
            case:gpt-4-0314
            case:gpt-4o
            case:gpt-4o-2024-05-13
               math.divide
                  math.subtract:int:8192
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-3.5-turbo-0125
               math.divide
                  math.subtract:int:16384
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4-32k
            case:gpt-4-32k-0314
               math.divide
                  math.subtract:int:32768
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4-0125-preview
            case:gpt-4-1106-preview
               math.divide
                  math.subtract:int:131072
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:o3-mini
               math.divide
                  math.subtract:int:200000
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            default
               math.divide
                  math.subtract:int:2049
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

   // Making sure prompt is not larger than [max_request_tokens].
   if
      mt
         openai.tokenize:x:@.arguments/*/prompt
         convert:x:@.model/*/max_request_tokens
            type:int
      .lambda

         // Oops, more prompt than model allows for.
         throw:Your request is longer than what this type is configured to allow for
            public:bool:true
            status:int:400

   // Invoking callback provided by caller.
   add:x:./*/invoke
      get-nodes:x:@.arguments/*
      get-nodes:x:@.model/*
   remove-nodes:x:./*/invoke/*/.callback
   invoke:x:@.arguments/*/.callback

   // Updating [no_requests] on model.
   data.connect:[generic|magic]
      data.execute:update ml_types set no_requests = no_requests + 1 where type = @type
         type:x:@.arguments/*/type

   // Returning result of invocation to caller.
   return-nodes:x:@invoke/*
