
// Write a Hyperlambda script for a completion type of slot for non-GPT type of models.
slots.create:magic.ai.completion

   /*
    * Checking if model is configured to return cached requests.
    * 
    * Notice, the type needs to have turned on cache, and we need to have a
    * historical request that's cached matching the prompt and the type,
    * and caller must not have asked for references for the cache to kick in.
    */
   if
      and
         exists:x:@.arguments/*/cached
         not-null:x:@.arguments/*/cached
         eq
            convert:x:@.arguments/*/cached
               type:int
            .:int:1
         or
            not-exists:x:@.arguments/*/references
            not
               get-value:x:@.arguments/*/references
      .lambda

         // Checking if we can find a cached request matching prompt and type.
         data.connect:[generic|magic]
            data.read
               table:ml_requests
               columns
                  completion
                  cached
               where
                  and
                     prompt.eq:x:@.arguments/*/prompt
                     type.eq:x:@.arguments/*/type
               limit:1
               order:created
               direction:desc

            // Checking if we could find a matching request in cache.
            if
               and
                  exists:x:@data.read/*/*
                  eq
                     convert:x:@data.read/*/*/cached
                        type:int
                     .:int:1
               .lambda

                  // Returning cached completion.
                  unwrap:x:+/*
                  return
                     result:x:@data.read/*/*/completion
                     finish_reason:cached

   // Result we're returning to caller.
   .result

   // Prompt that might be changed if model contains a prefix, and/or we're using embeddings.
   .prompt

   /*
    * Checking if model is configured to use embeddings.
    * 
    * If model is configured to using embeddings, we find relevant training snippets from
    * training data and use as "CONTEXT". If not, it might be either a fine-tuned model,
    * or a "mint" model, where mint implies no context or customisation having been done,
    * which is true if user just wants the default AI model without any customisations.
    */
   if
      eq
         convert:x:@.arguments/*/use_embeddings
            type:int
         .:int:1
      .lambda

         // Retrieving relevant snippets.
         unwrap:x:+/*
         signal:magic.ai.get-context
            type:x:@.arguments/*/type
            prompt:x:@.arguments/*/prompt
            threshold:x:@.arguments/*/threshold
            max_tokens:x:@.arguments/*/max_context_tokens
            api_key:x:@.arguments/*/api_key

         // Checking if we've got a cached result.
         if
            exists:x:@signal/*/cached
            .lambda

               // Cached result.
               unwrap:x:+/*
               return
                  result:x:@signal/*/cached
                  finish_reason:cached

         // Adding db_time to [.result].
         add:x:@.result
            get-nodes:x:@signal/*/db_time

         // Correctly applying prefix.
         if
            and
               strings.contains:x:@.arguments/*/prefix
                  .:[CONTEXT]
               strings.contains:x:@.arguments/*/prefix
                  .:[QUESTION]
               strings.contains:x:@.arguments/*/prefix
                  .:"ANSWER:"
            .lambda

               // Structured prefix.
               set-value:x:@.prompt
                  strings.replace:x:@.arguments/*/prefix
                     .:[QUESTION]
                     get-value:x:@.arguments/*/prompt
               set-value:x:@.prompt
                  strings.replace:x:@.prompt
                     .:[CONTEXT]
                     get-value:x:@signal/*/context
         else

            // Old style prefix.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prefix
                  .:"\nQUESTION: "
                  get-value:x:@.arguments/*/prompt
                  .:"\nCONTEXT: \n"
                  get-value:x:@signal/*/context
                  .:"\nANSWER: "

         // Checking if caller wants references.
         if
            and
               exists:x:@.arguments/*/references
               not-null:x:@.arguments/*/references
               get-value:x:@.arguments/*/references
            .lambda
               add:x:@.result
                  .
                     references
               add:x:@.result/*/references
                  get-nodes:x:@signal/*/snippets/*
   else

      /*
       * Not using embeddings, either a mint model or a fine-tuned model.
       * 
       * Checking if model is fine-tuned, which is true if model name contains
       * a colon (:), at which point we apply "->" and " END" to help model roundtrip.
       */
      if
         strings.contains:x:@.arguments/*/model
            .:":"
         .lambda

            // Fine-tuned model, making sure we apply STOP characters to prompt and invocation.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prompt
                  .:" ->"
            add:x:../*/http.post/*/payload
               .
                  stop:" END"
      else

         // "Mint" model - Implying default model without any customisations.
         set-value:x:@.prompt
            get-value:x:@.arguments/*/prompt

   // Retrieving token.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         get-first-value
            get-value:x:@.arguments/*/api_key
            config.get:"magic:openai:key"

   // Invoking OpenAI, now with a decorated prompt.
   http.post:"https://api.openai.com/v1/completions"
      headers
         Authorization:x:@.token
         Content-Type:application/json
      payload
         prompt:x:@.prompt
         model:x:@.arguments/*/model
         max_tokens:x:@.arguments/*/max_tokens
         temperature:x:@.arguments/*/temperature
      convert:true

   // Sanity checking above invocation.
   if
      not
         and
            mte:x:@http.post
               .:int:200
            lt:x:@http.post
               .:int:300
      .lambda

         // Oops, error - Logging error and returning status 500 to caller.
         lambda2hyper:x:@http.post
         log.error:Something went wrong while invoking OpenAI
            message:x:@http.post/*/content/*/error/*/message
            status:x:@http.post
            error:x:@lambda2hyper
         response.status.set:x:@http.post
         unwrap:x:+/*
         return
            error:bool:true
            result:x:@http.post/*/content/*/error/*/message
   else

      // Success! Logging as such!
      log.info:Invoking OpenAI was a success

   // Making sure we trim response before adding it to [.result].
   strings.trim:x:@http.post/*/content/*/choices/0/*/text
   unwrap:x:+/*/*
   add:x:@.result
      .
         result:x:@strings.trim

   // Making sure we return finish reason to caller.
   get-first-value
      get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
      .:unknown
   unwrap:x:+/*/*
   add:x:@.result
      .
         finish_reason:x:@get-first-value

   // Re-opening database connection since it might have timed out while we're waiting for OpenAI.
   data.connect:[generic|magic]

      // Checking if type is 'supervised', at which point we store prompt and completion.
      if
         and
            not
               get-value:x:@.result/*/error
            not
               exists:x:@data.read/*/*
            mt
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:0
         .lambda

            // Storing prompt and completion in ml_requests table.
            data.create
               table:ml_requests
               values
                  type:x:@.arguments/*/type
                  prompt:x:@.arguments/*/prompt
                  completion:x:@.result/*/result
                  finish_reason:x:@.result/*/finish_reason
                  session:x:@.arguments/*/session
                  user_id:x:@.arguments/*/user_id
                  referrer:x:@.arguments/*/referrer

   /*
    * Applying some HTTP caching to avoid invoking OpenAI again with
    * the same question before some minimum amount of time has passed.
    */
   response.headers.set
      Cache-Control:max-age=30

   // Returning results returned from invocation above to caller.
   add:x:+
      get-nodes:x:@.result/*/result
      get-nodes:x:@.result/*/finish_reason
      get-nodes:x:@.result/*/references
      get-nodes:x:@.result/*/db_time
   return
